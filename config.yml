## Model and tokenizer.
pretrained_model_name: "kakaobrain/kogpt"
revision: "KoGPT6B-ryan1.5b-float16"

## Dataset.
batch_size: 1
num_workers: 16

## Logger.
wandb_project: "mrt"

## Trainer.
accelerator: "gpu"
devices: 2
strategy: "deepspeed_stage_3_offload"
precision: "16-mixed"
accumulate_grad_batches: 32
max_steps: 1_000
logging_interval: 10
ckpt: "ckpt"

## Optimizer.
lr: 0.00015

## Generation.
temperature: 1
repetition_penalty: 1
min_length: 8
max_length: 8
top_p: 0.95
top_k: 40
no_repeat_ngram_size: 3

## MRT.
rl_n_samples: 1

## Model and tokenizer.
# pretrained_model_name: "kakaobrain/kogpt"
# pretrained_model_name: "EleutherAI/polyglot-ko-1.3b"
pretrained_model_name: "EleutherAI/gpt-neo-1.3B"
# revision: "KoGPT6B-ryan1.5b-float16"
revision: "main"

## Dataset.
samples_per_epoch: 10_00
batch_size: 64
num_workers: 24

## Logger.
wandb_project: "mrt"

## Callbacks.
ckpt: "ckpt"
every_n_epochs: 1
save_top_k: -1

## Trainer.
buffer_size: 1_00
accelerator: "gpu"
devices: 2
precision: "16-mixed"
accumulate_grad_batches: 4
max_epochs: 1
# max_steps: 10_000
logging_interval: 1
detect_anomaly: False

## Optimizer.
lr: 0.00002

## Generation.
do_sample: True
min_new_tokens: 64
max_new_tokens: 64
no_repeat_ngram_size: 3
top_p: 0.95
top_k: 40

## MRT.
rl_n_samples: 1

## Debug.
debug: True